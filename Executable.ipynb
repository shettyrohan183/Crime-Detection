{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c89476c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "def some_magic(string):\n",
    "    def warn(*args, **kwargs):\n",
    "        pass\n",
    "    import warnings\n",
    "    warnings.warn = warn\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    from tqdm import tqdm\n",
    "    import numpy as np\n",
    "    import os\n",
    "    import time\n",
    "    from time import time\n",
    "    import cv2\n",
    "    import firebase_admin\n",
    "    from firebase_admin import credentials , firestore ,db\n",
    "\n",
    "    cred = credentials.Certificate(\"data.json\")\n",
    "\n",
    "\n",
    "    firebase_admin.initialize_app(cred,{\n",
    "    'databaseURL': 'https://crimedetect-f51eb-default-rtdb.firebaseio.com/'\n",
    "    })\n",
    "    import tensorflow\n",
    "    from tensorflow.keras.utils import Sequence\n",
    "    from tensorflow.keras import utils\n",
    "    from tensorflow.keras.utils import to_categorical\n",
    "    from tensorflow.python.keras.models  import Sequential, Input, Model\n",
    "    from tensorflow.keras.layers import Dense, Flatten, Conv3D, MaxPooling3D, Dropout, BatchNormalization, Activation, LeakyReLU, Add, Multiply\n",
    "    from tensorflow.keras.regularizers import l2\n",
    "    # from keras.layers.core import Lambda\n",
    "    from tensorflow.keras.layers import Lambda\n",
    "    # from keras.utils import np_utils\n",
    "\n",
    "    class DataGenerator(Sequence):\n",
    "        \"\"\"Data Generator inherited from keras.utils.Sequence\n",
    "        Args: \n",
    "            directory: the path of data set, and each sub-folder will be assigned to one class\n",
    "            batch_size: the number of data points in each batch\n",
    "            shuffle: whether to shuffle the data per epoch\n",
    "        Note:\n",
    "            If you want to load file with other data format, please fix the method of \"load_data\" as you want\n",
    "        \"\"\"\n",
    "        def __init__(self, directory, batch_size=1, shuffle=True, data_augmentation=True):\n",
    "            # Initialize the params\n",
    "            self.batch_size = batch_size\n",
    "            self.directory = directory\n",
    "            self.shuffle = shuffle\n",
    "            self.data_aug = data_augmentation\n",
    "            # Load all the save_path of files, and create a dictionary that save the pair of \"data:label\"\n",
    "            self.X_path, self.Y_dict = self.search_data() \n",
    "            # Print basic statistics information\n",
    "            self.print_stats()\n",
    "            return None\n",
    "        def search_data(self):\n",
    "            X_path = []\n",
    "            Y_dict = {}\n",
    "            # list all kinds of sub-folders\n",
    "            self.dirs = sorted(os.listdir(self.directory))\n",
    "            one_hots = utils.to_categorical(range(len(self.dirs)))\n",
    "            for i,folder in enumerate(self.dirs):\n",
    "                folder_path = os.path.join(self.directory,folder)\n",
    "                for file in os.listdir(folder_path):\n",
    "                    file_path = os.path.join(folder_path,file)\n",
    "                    # append the each file path, and keep its label  \n",
    "                    X_path.append(file_path)\n",
    "                    Y_dict[file_path] = one_hots[i]\n",
    "            return X_path, Y_dict\n",
    "    \n",
    "        def print_stats(self):\n",
    "            # calculate basic information\n",
    "            self.n_files = len(self.X_path)\n",
    "            self.n_classes = len(self.dirs)\n",
    "            self.indexes = np.arange(len(self.X_path))\n",
    "            np.random.shuffle(self.indexes)\n",
    "            # Output states\n",
    "            print(\"Found {} files belonging to {} classes.\".format(self.n_files,self.n_classes))\n",
    "            for i,label in enumerate(self.dirs):\n",
    "                print('%10s : '%(label),i)\n",
    "            return None\n",
    "        def __len__(self):\n",
    "            # calculate the iterations of each epoch\n",
    "            steps_per_epoch = np.ceil(len(self.X_path) / float(self.batch_size))\n",
    "            return int(steps_per_epoch)\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "            \"\"\"Get the data of each batch\n",
    "            \"\"\"\n",
    "            # get the indexs of each batch\n",
    "            batch_indexs = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "            # using batch_indexs to get path of current batch\n",
    "            batch_path = [self.X_path[k] for k in batch_indexs]\n",
    "            # get batch data\n",
    "            batch_x, batch_y = self.data_generation(batch_path)\n",
    "            return batch_x, batch_y\n",
    "\n",
    "        def on_epoch_end(self):\n",
    "            # shuffle the data at each end of epoch\n",
    "            if self.shuffle == True:\n",
    "                np.random.shuffle(self.indexes)\n",
    "\n",
    "        def data_generation(self, batch_path):\n",
    "            # load data into memory, you can change the np.load to any method you want\n",
    "            batch_x = [self.load_data(x) for x in batch_path]\n",
    "            batch_y = [self.Y_dict[x] for x in batch_path]\n",
    "            # transfer the data format and take one-hot coding for labels\n",
    "            batch_x = np.array(batch_x)\n",
    "            batch_y = np.array(batch_y)\n",
    "            return batch_x, batch_y\n",
    "    \n",
    "        def normalize(self, data):\n",
    "            mean = np.mean(data)\n",
    "            std = np.std(data)\n",
    "            return (data-mean) / std\n",
    "    \n",
    "        def random_flip(self, video, prob):\n",
    "            s = np.random.rand()\n",
    "            if s < prob:\n",
    "                video = np.flip(m=video, axis=2)\n",
    "            return video    \n",
    "    \n",
    "        def uniform_sampling(self, video, target_frames=64):\n",
    "            # get total frames of input video and calculate sampling interval \n",
    "            len_frames = int(len(video))\n",
    "            interval = int(np.ceil(len_frames/target_frames))\n",
    "            # init empty list for sampled video and \n",
    "            sampled_video = []\n",
    "            for i in range(0,len_frames,interval):\n",
    "                sampled_video.append(video[i])     \n",
    "            # calculate numer of padded frames and fix it \n",
    "            num_pad = target_frames - len(sampled_video)\n",
    "            padding = []\n",
    "            if num_pad>0:\n",
    "                for i in range(-num_pad,0):\n",
    "                    try: \n",
    "                        padding.append(video[i])\n",
    "                    except:\n",
    "                        padding.append(video[0])\n",
    "                sampled_video += padding     \n",
    "            # get sampled video\n",
    "            return np.array(sampled_video, dtype=np.float32)\n",
    "    \n",
    "        def random_clip(self, video, target_frames=64):\n",
    "            start_point = np.random.randint(len(video)-target_frames)\n",
    "            return video[start_point:start_point+target_frames]\n",
    "    \n",
    "        def dynamic_crop(self, video):\n",
    "            # extract layer of optical flow from video\n",
    "            opt_flows = video[...,3]\n",
    "            # sum of optical flow magnitude of individual frame\n",
    "            magnitude = np.sum(opt_flows, axis=0)\n",
    "            # filter slight noise by threshold \n",
    "            thresh = np.mean(magnitude)\n",
    "            magnitude[magnitude<thresh] = 0\n",
    "            # calculate center of gravity of magnitude map and adding 0.001 to avoid empty value\n",
    "            x_pdf = np.sum(magnitude, axis=1) + 0.001\n",
    "            y_pdf = np.sum(magnitude, axis=0) + 0.001\n",
    "            # normalize PDF of x and y so that the sum of probs = 1\n",
    "            x_pdf /= np.sum(x_pdf)\n",
    "            y_pdf /= np.sum(y_pdf)\n",
    "            # randomly choose some candidates for x and y \n",
    "            x_points = np.random.choice(a=np.arange(224), size=10, replace=True, p=x_pdf)\n",
    "            y_points = np.random.choice(a=np.arange(224), size=10, replace=True, p=y_pdf)\n",
    "            # get the mean of x and y coordinates for better robustness\n",
    "            x = int(np.mean(x_points))\n",
    "            y = int(np.mean(y_points))\n",
    "            # avoid to beyond boundaries of array\n",
    "            x = max(56,min(x,167))\n",
    "            y = max(56,min(y,167))\n",
    "            # get cropped video \n",
    "            return video[:,x-56:x+56,y-56:y+56,:] \n",
    "\n",
    "        def color_jitter(self,video):\n",
    "            # range of s-component: 0-1\n",
    "            # range of v component: 0-255\n",
    "            s_jitter = np.random.uniform(-0.2,0.2)\n",
    "            v_jitter = np.random.uniform(-30,30)\n",
    "            for i in range(len(video)):\n",
    "                hsv = cv2.cvtColor(video[i], cv2.COLOR_RGB2HSV)\n",
    "                s = hsv[...,1] + s_jitter\n",
    "                v = hsv[...,2] + v_jitter\n",
    "                s[s<0] = 0\n",
    "                s[s>1] = 1\n",
    "                v[v<0] = 0\n",
    "                v[v>255] = 255\n",
    "                hsv[...,1] = s\n",
    "                hsv[...,2] = v\n",
    "                video[i] = cv2.cvtColor(hsv, cv2.COLOR_HSV2RGB)\n",
    "            return video\n",
    "\n",
    "        def load_data(self, path):\n",
    "            # load the processed .npy files which have 5 channels (1-3 for RGB, 4-5 for optical flows)\n",
    "            data = np.load(path, mmap_mode='r',allow_pickle=True)\n",
    "            data = np.float32(data)\n",
    "            # sampling 64 frames uniformly from the entire video\n",
    "            data = self.uniform_sampling(video=data, target_frames=64)\n",
    "            # whether to utilize the data augmentation\n",
    "            if  self.data_aug:\n",
    "                data[...,:3] = self.color_jitter(data[...,:3])\n",
    "                data = self.random_flip(data, prob=0.5)\n",
    "            # normalize rgb images and optical flows, respectively\n",
    "            data[...,:3] = self.normalize(data[...,:3])\n",
    "            data[...,3:] = self.normalize(data[...,3:])\n",
    "            return data\n",
    "\n",
    "    def Video2Npy(file_path, resize=(224,224)):\n",
    "        \"\"\"Load video and tansfer it into .npy format\n",
    "        Args:\n",
    "            file_path: the path of video file\n",
    "            resize: the target resolution of output video\n",
    "        Returns:\n",
    "            frames: gray-scale video\n",
    "            flows: magnitude video of optical flows \n",
    "        \"\"\"\n",
    "        # Load video\n",
    "        cap = cv2.VideoCapture(file_path)\n",
    "        # Get number of frames\n",
    "        len_frames = int(cap.get(7))\n",
    "        # Extract frames from video\n",
    "        try:\n",
    "            frames = []\n",
    "            for i in range(len_frames-1):\n",
    "                _, frame = cap.read()\n",
    "                frame = cv2.resize(frame,resize, interpolation=cv2.INTER_AREA)\n",
    "                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                frame = np.reshape(frame, (224,224,3))\n",
    "                frames.append(frame)   \n",
    "        except:\n",
    "            print(\"Error: \", file_path, len_frames,i)\n",
    "        finally:\n",
    "            frames = np.array(frames)\n",
    "            cap.release()\n",
    "\n",
    "        # Get the optical flow of video\n",
    "        flows = getOpticalFlow(frames)\n",
    "\n",
    "        result = np.zeros((len(flows),224,224,5))\n",
    "        result[...,:3] = frames\n",
    "        result[...,3:] = flows\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def getOpticalFlow(video):\n",
    "        \"\"\"Calculate dense optical flow of input video\n",
    "        Args:\n",
    "            video: the input video with shape of [frames,height,width,channel]. dtype=np.array\n",
    "        Returns:\n",
    "            flows_x: the optical flow at x-axis, with the shape of [frames,height,width,channel]\n",
    "            flows_y: the optical flow at y-axis, with the shape of [frames,height,width,channel]\n",
    "        \"\"\"\n",
    "        # initialize the list of optical flows\n",
    "        gray_video = []\n",
    "        for i in range(len(video)):\n",
    "            img = cv2.cvtColor(video[i], cv2.COLOR_RGB2GRAY)\n",
    "            gray_video.append(np.reshape(img,(224,224,1)))\n",
    "\n",
    "        flows = []\n",
    "        for i in range(0,len(video)-1):\n",
    "            # calculate optical flow between each pair of frames\n",
    "            flow = cv2.calcOpticalFlowFarneback(gray_video[i], gray_video[i+1], None, 0.5, 3, 15, 3, 5, 1.2, cv2.OPTFLOW_FARNEBACK_GAUSSIAN)\n",
    "            # subtract the mean in order to eliminate the movement of camera\n",
    "            flow[..., 0] -= np.mean(flow[..., 0])\n",
    "            flow[..., 1] -= np.mean(flow[..., 1])\n",
    "            # normalize each component in optical flow\n",
    "            flow[..., 0] = cv2.normalize(flow[..., 0],None,0,255,cv2.NORM_MINMAX)\n",
    "            flow[..., 1] = cv2.normalize(flow[..., 1],None,0,255,cv2.NORM_MINMAX)\n",
    "            # Add into list \n",
    "            flows.append(flow)\n",
    "\n",
    "        # Padding the last frame as empty array\n",
    "        flows.append(np.zeros((224,224,2)))\n",
    "\n",
    "        return np.array(flows, dtype=np.float32)\n",
    "\n",
    "\n",
    "    def Save2Npy(file_dir, save_dir):\n",
    "        \"\"\"Transfer all the videos and save them into specified directory\n",
    "        Args:\n",
    "            file_dir: source folder of target videos\n",
    "            save_dir: destination folder of output .npy files\n",
    "        \"\"\"\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "        # List the files\n",
    "        videos = os.listdir(file_dir)\n",
    "        for v in tqdm(videos):\n",
    "            # Split video name\n",
    "            video_name = v.split('.')[0]\n",
    "            # Get src \n",
    "            video_path = os.path.join(file_dir, v)\n",
    "            # Get dest \n",
    "            save_path = os.path.join(save_dir, video_name+'.npy') \n",
    "            # Load and preprocess video\n",
    "            data = Video2Npy(file_path=video_path, resize=(224,224))\n",
    "            data = np.uint8(data)\n",
    "            # Save as .npy file\n",
    "            np.save(save_path, data)\n",
    "\n",
    "        return None\n",
    "        \n",
    "        \n",
    "    from os import listdir\n",
    "    from os.path import isfile, join\n",
    "    if string==\"web\":\n",
    "        \n",
    "\n",
    "        # The duration in seconds of the video captured\n",
    "        for i in range(2):\n",
    "            capture_duration = 15\n",
    "\n",
    "            cap = cv2.VideoCapture(0)\n",
    "\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "            out = cv2.VideoWriter(r'E:\\go_ai\\last-crime\\ourliv\\12-0362_77-4886'+str(i)+'.avi',fourcc, 20.0, (640,480))\n",
    "\n",
    "            start_time = time.time()\n",
    "            while( int(time.time() - start_time) < capture_duration ):\n",
    "                ret, frame = cap.read()\n",
    "                if ret==True:\n",
    "                    frame = cv2.flip(frame,0)\n",
    "                    out.write(frame)\n",
    "                    cv2.imshow('frame',frame)\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            cap.release()\n",
    "            out.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            \n",
    "#         source_path = r'E:\\go_ai\\last-crime\\ourliv'\n",
    "#         target_path = r'E:\\go_ai\\last-crime\\data\\our\\vid'\n",
    "#         Save2Npy(source_path, target_path)\n",
    "        onlyfiles = [f for f in listdir(r'E:\\go_ai\\last-crime\\data\\our\\vid') if isfile(join(r'E:\\go_ai\\last-crime\\data\\our\\vid', f))]\n",
    "        list = os.listdir(r'E:\\go_ai\\last-crime\\data\\our\\vid') # dir is your directory path\n",
    "        number_files = len(list)\n",
    "        print(number_files)\n",
    "        val_generator = DataGenerator(directory=r'E:\\go_ai\\last-crime\\data\\our',\n",
    "                              batch_size=1, \n",
    "                              data_augmentation=False)\n",
    "    if string==\"local\":\n",
    "        source_path = r'E:\\go_ai\\last-crime\\video'\n",
    "        target_path = r'E:\\go_ai\\last-crime\\data\\sam\\vi'\n",
    "        Save2Npy(source_path, target_path)\n",
    "        onlyfiles = [f for f in listdir(r'E:\\go_ai\\last-crime\\data\\sam\\vi') if isfile(join(r'E:\\go_ai\\last-crime\\data\\sam\\vi', f))]\n",
    "        list = os.listdir(r'E:\\go_ai\\last-crime\\data\\sam\\vi') # dir is your directory path\n",
    "        number_files = len(list)\n",
    "        print(number_files)\n",
    "        val_generator = DataGenerator(directory=r'E:\\go_ai\\last-crime\\data\\sam',\n",
    "                              batch_size=1, \n",
    "                              data_augmentation=False)\n",
    "        \n",
    "    def addData(lat_lang,time):\n",
    "        ref = db.reference(\"/\")\n",
    "        res=ref.get(\"/\")\n",
    "        print(res[0])\n",
    "        res[0][lat_lang]=time\n",
    "        res=ref.set(res[0])\n",
    "        \n",
    "    from pickle import load\n",
    "    from tensorflow.keras.models import load_model\n",
    "    hist = load_model('keras_model.h5')\n",
    "        \n",
    "    import datetime\n",
    "    pred = hist.predict(val_generator)\n",
    "    \n",
    "    print(pred)\n",
    "    \n",
    "    for i in range(number_files):\n",
    "        if pred[i][0] > pred[i][1]:\n",
    "            e = datetime.datetime.now()\n",
    "            date_time = e.strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "            print(\"date and time:\", date_time)\n",
    "            if(string==\"web\"):\n",
    "                name = onlyfiles[i].replace(\".npy\",\"\")\n",
    "            else:\n",
    "                name = onlyfiles[i].replace(\".npy\",\"\")\n",
    "        print(name,\" crime has been detected at \",e)\n",
    "        addData(name,date_time)\n",
    "            \n",
    "            \n",
    "#     import os\n",
    "#     if string==\"web\":\n",
    "#         dir = 'E:\\go_ai\\last-crime\\data\\our\\vid'\n",
    "#     else:\n",
    "#         dir = 'E:\\go_ai\\last-crime\\data\\sam\\vi'\n",
    "#     for f in os.listdir(dir):\n",
    "#         os.remove(os.path.join(dir, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2379f24b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The default Firebase app already exists. This means you called initialize_app() more than once without providing an app name as the second argument. In most cases you only need to call initialize_app() once. But if you do want to initialize multiple apps, pass a second argument to initialize_app() to give each app a unique name.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43msome_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocal\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36msome_magic\u001b[1;34m(string)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfirebase_admin\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m credentials , firestore ,db\n\u001b[0;32m     17\u001b[0m cred \u001b[38;5;241m=\u001b[39m credentials\u001b[38;5;241m.\u001b[39mCertificate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 20\u001b[0m \u001b[43mfirebase_admin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize_app\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdatabaseURL\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttps://crimedetect-f51eb-default-rtdb.firebaseio.com/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[0;32m     22\u001b[0m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequence\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\firebase_admin\\__init__.py:71\u001b[0m, in \u001b[0;36minitialize_app\u001b[1;34m(credential, options, name)\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m app\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m _DEFAULT_APP_NAME:\n\u001b[1;32m---> 71\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m((\n\u001b[0;32m     72\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe default Firebase app already exists. This means you called \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     73\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minitialize_app() more than once without providing an app name as \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     74\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe second argument. In most cases you only need to call \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     75\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minitialize_app() once. But if you do want to initialize multiple \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     76\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mapps, pass a second argument to initialize_app() to give each app \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     77\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma unique name.\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m((\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFirebase app named \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m already exists. This means you called \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minitialize_app() more than once with the same app name as the \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msecond argument. Make sure you provide a unique name every time \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myou call initialize_app().\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mformat(name))\n",
      "\u001b[1;31mValueError\u001b[0m: The default Firebase app already exists. This means you called initialize_app() more than once without providing an app name as the second argument. In most cases you only need to call initialize_app() once. But if you do want to initialize multiple apps, pass a second argument to initialize_app() to give each app a unique name."
     ]
    }
   ],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     some_magic(\"local\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f61b064",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bbbd0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "5ae58900cfbb8c43ab3495913814b7cf26024f51651a94ce8bf64d6111688e8d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
